# -*- coding: utf-8 -*-
"""Obesity Classification-DecisionTree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yk1-F-Y65_c7bAiXZHAU_YNIB9ehMf4K

##Introduction of the Dataset
####The Obesity Classification Dataset is a dataset commonly used in machine learning for classification tasks, particularly in the domain of healthcare and medical research. This dataset typically contains various features related to individuals' lifestyles, eating habits, physical activity levels, and health indicators, such as: Age, Gender, Weight, BMI and Lable(Obesity Level)

###This dataset contains information about the obesity classification of individuals. The data was collected from a variety of sources, including medical records, surveys, and self-reported data. The dataset includes the following columns:

* ID: A unique identifier for each individual
* Age: The age of the individual
* Gender: The gender of the individual
* Height: The height of the individual in centimeters
* Weight: The weight of the individual in kilograms
* BMI: The body mass index of the individual, calculated as weight divided by height squared
* Label: The obesity classification of the individual, which can be one of the following:
- Normal Weight
- Overweight
- Obese
- Underweight

##Source

https://www.kaggle.com/datasets/sujithmandala/obesity-classification-dataset

##Importing Necessary Libraries
"""

import pandas as pd #Pandas is a powerful library for data manipulation and analysis.
import numpy as np #NumPy is a powerful tool for numerical computations in Python.
import matplotlib.pyplot as plt #Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python
import sklearn as dataset #sklearn library, which provides tools for data preprocessing, model building, and evaluation in machine learning.
from sklearn.model_selection import train_test_split #Import function to split data into training and testing sets
from sklearn.tree import DecisionTreeClassifier #Import the decision tree classifier for building the model
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  #Import metrics for evaluating model performance

"""##Loading the Dataset"""

df= pd.read_csv("Obesity Classifications.csv") #Loading the dataset.

"""##Analyze the dataset"""

df.head() #Displays the first 5 rows of the dataset.

"""##Exploring the Data:
###Understanding the dataset by exploring its structure and contents.
"""

df.columns #Displays the names of the columns

df.shape # Displays the total count of the Rows and Columns respectively.

df.info() #Displays the total count of values present in the particular column along with the null count and data type.

"""##Data Cleaning:
###Checking for missing values, duplicates, or any inconsistencies and clean the data accordingly.
"""

df.isnull().sum() #Checking for any null data.

"""###There is no null/empty data in the dataset"""

df.duplicated().sum()  #Checking for any Duplicate Data

"""###There is no duplicate value in the dataset"""

df.describe(include="all")

df.drop(columns=['ID'], inplace=True) #Drop the 'ID' column, as it is not useful

df.head()

# @title Label vs Age

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(df['Label'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(df, x='Age', y='Label', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

"""###Decision tree algorithms typically require these categorical variables to be encoded into numerical representations before they can be used in the model.
Label encoding for the 'Gender' column and 'Label' column
"""

from sklearn.preprocessing import LabelEncoder

# Label encoding for the 'Gender' column
gender_encoder = LabelEncoder()
df['Gender_encoded'] = gender_encoder.fit_transform(df['Gender'])

# Label encoding for the 'Label' column
label_encoder = LabelEncoder()
df['Label_encoded'] = label_encoder.fit_transform(df['Label'])

df.head()

# Drop the 'Gender and label' column as we do not need the categorical values
df.drop(columns=['Gender','Label'], inplace=True)
df.head()

df.iloc[:, 0:4]  # Select all rows and the first four columns from the DataFrame

x = df.iloc[:,0:6] #This selects all rows and the first six columns from the DataFrame df.
y = df.iloc[:,5] #This selects all rows and the 6th column (index 5) from the DataFrame df.

x #The resulting DataFrame x contains the features for the model.

y #The resulting Series y contains the target variable for the model.

"""##Splitting Data for traing the model"""

x=df.drop('Label_encoded',axis=1)
y=df.Label_encoded

"""###x = df.drop('Label_encoded', axis=1):

###This removes the column named 'Label_encoded' from the DataFrame df and assigns the resulting DataFrame to x.
###The resulting DataFrame x contains all the features except the target variable.
###axis=1 specifies that we are dropping a column (if it were axis=0, it would drop rows).

###y = df.Label_encoded:

###This selects the column named 'Label_encoded' from the DataFrame df and assigns it to y.
###The resulting Series y contains the target variable for the model.

###By doing this, x holds all the features needed for model training, and y holds the target labels the model aims to predict.
"""

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Initialize the decision tree classifier
dt_classifier = DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, criterion='gini')

# Fit the classifier to the training data
dt_classifier.fit(x_train, y_train)

# Predict on the testing data
y_pred = dt_classifier.predict(x_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Precision
precision = precision_score(y_test, y_pred, average='weighted')
print("Precision:", precision)

# Recall
recall = recall_score(y_test, y_pred, average='weighted')
print("Recall:", recall)

# F1-score
f1 = f1_score(y_test, y_pred, average='weighted')
print("F1-score:", f1)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""####Accuracy, Precision, Recall, and F1-score: The model achieved perfect scores of 1.0 for accuracy, precision, recall, and F1-score. This indicates that the model made no errors in classifying the test data. A perfect score suggests that the model performed exceptionally well and achieved optimal classification performance.
####Classification Report: The classification report provides a detailed breakdown of the model's performance for each class. For all classes (0, 1, 2, and 3), the precision, recall, and F1-score are 1.0, indicating perfect classification performance. The support column shows the number of samples for each class in the test set.
####Conclusion: The results indicate that the decision tree classifier performed flawlessly on the test data, achieving perfect classification accuracy for all classes. This suggests that the model was able to effectively learn and generalize the underlying patterns in the data. However, it's essential to consider the possibility of overfitting, especially when the model performs exceptionally well on the test data.

##Plotting the Decision Tree
"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Convert class names to strings
class_names = dt_classifier.classes_.astype(str)

# Plot the decision tree
plt.figure(figsize=(20, 10))
plot_tree(dt_classifier, feature_names=x.columns, class_names=class_names, filled=True)
plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.model_selection import cross_val_score

# Perform cross-validation
scores = cross_val_score(dt_classifier, x, y, cv=5, scoring='accuracy')

# Print cross-validation scores
print("Cross-validation scores:", scores)

# Calculate and print mean cross-validation score
mean_score = scores.mean()
print("Mean cross-validation score:", mean_score)

"""####Mean Cross-Validation Score: The mean cross-validation score is approximately 0.954, indicating that, on average, the decision tree classifier achieved an accuracy of around 95.4% across the different folds. This suggests that the model performs well and consistently on unseen data.

####Cross-Validation Scores: The individual cross-validation scores range from 86.4% to 100%. While most of the folds achieved high accuracy (ranging from 90.5% to 100%), one fold had a slightly lower accuracy of 86.4%. Overall, the model demonstrates good generalization performance across different subsets of the data.

####Conclusion: The cross-validation results support the effectiveness and generalization ability of the decision tree classifier. The model's performance is consistent across multiple folds, indicating its reliability in classifying unseen data. However, it's essential to consider potential variations in performance across different subsets of the data
"""

